# -*- coding: utf-8 -*-
"""CV/NLP Assignment Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TAKLuwxcBgnZngVLW2KeWEKbaHygZka4
"""

import kagglehub

# Download latest version
data_path = kagglehub.dataset_download("meowmeowmeowmeowmeow/gtsrb-german-traffic-sign")

print("Path to dataset files:", data_path)

# Commented out IPython magic to ensure Python compatibility.
#Importing Libraries
# Fundamental classes
import numpy as np
import pandas as pd
import tensorflow as tf
import os

#Preprocessing
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.utils.class_weight import compute_class_weight

# Image related
import cv2
from PIL import Image

# Performance Plot
from sklearn import metrics
from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report, precision_score, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

# For the model and it's training
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Time
import time
import datetime

#For testing
import random

# Load the CSV files
meta_df = pd.read_csv('/root/.cache/kagglehub/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign/versions/1/Meta.csv')
train_df = pd.read_csv('/root/.cache/kagglehub/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign/versions/1/Train.csv')
test_df = pd.read_csv('/root/.cache/kagglehub/datasets/meowmeowmeowmeowmeow/gtsrb-german-traffic-sign/versions/1/Test.csv')

print(meta_df.head())
print(train_df.head())
print(test_df.head())

# Define a function to load and preprocess images from a DataFrame
def load_and_preprocess_images(df, image_size=(30, 30)):
    images = []  # Initialize an empty list to store image data
    labels = []  # Initialize an empty list to store image labels (classes)

    # Iterate over each row in the DataFrame
    for index, row in df.iterrows():
        # Construct the full image file path
        img_path = os.path.join(data_path, row['Path'])
        # Load the image and resize it to the specified dimensions
        img = load_img(img_path, target_size=image_size)
        # Convert the loaded image to a numpy array
        img_array = img_to_array(img)
        # Append the image array to the list of images
        images.append(img_array)
        # Append the corresponding label to the list of labels
        labels.append(row['ClassId'])

    # Convert lists of images and labels to numpy arrays and return them
    return np.array(images), np.array(labels)

# Load and preprocess the training images
X_train, y_train = load_and_preprocess_images(train_df)

# Load and preprocess the test images
X_test, y_test = load_and_preprocess_images(test_df)

# Normalize the training images by scaling pixel values to the range [0, 1]
X_train = X_train.astype('float32') / 255.0

# Normalize the test images by scaling pixel values to the range [0, 1]
X_test = X_test.astype('float32') / 255.0

# Encode labels to one-hot vectors
num_classes = meta_df['ClassId'].nunique()  # Determine the number of unique classes in the dataset
y_train_encoded = to_categorical(y_train, num_classes)  # Convert the training labels to one-hot encoded vectors
y_test_encoded = to_categorical(y_test, num_classes)  # Convert the test labels to one-hot encoded vectors

# Split the data into training and validation sets
X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(
    X_train, y_train_encoded,  # Features and labels for the training set
    test_size=0.2,  # Allocate 20% of the training data to the validation set
    random_state=42  # Set a seed for reproducibility of the data split
)

# Print the shapes of the resulting datasets to verify the split
print("Training set shape:", X_train_split.shape)  # Print the shape of the training features
print("Validation set shape:", X_val_split.shape)  # Print the shape of the validation features
print("Test set shape:", X_test.shape)  # Print the shape of the test features

#visulaize some training images
def visualize_images(images, labels, num_images=5):
    plt.figure(figsize=(15, 5))
    for i in range(num_images):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(images[i])
        plt.title(f"Label: {labels[i].argmax()}")
        plt.axis('off')
    plt.show()

visualize_images(X_train_split, y_train_split)

import matplotlib.pyplot as plt

def visualize_images(images, labels, num_images=5):
    plt.figure(figsize=(15, 5))
    for i in range(num_images):
        plt.subplot(1, num_images, i + 1)
        # Clip values to [0, 255] for display
        img_display = (images[i] * 300).astype(np.uint8)
        plt.imshow(img_display)
        plt.title(f"Label: {labels[i].argmax()}")
        plt.axis('off')
    plt.show()

# Visualize some training images
visualize_images(X_train_split, y_train_split)

# Data Augmentation to enhance the training dataset
datagen = ImageDataGenerator(
    rotation_range=10,  # Randomly rotate images by up to 10 degrees to introduce variety
    width_shift_range=0.1,  # Randomly shift images horizontally by up to 10% of the width to create variations
    height_shift_range=0.1,  # Randomly shift images vertically by up to 10% of the height to create variations
    zoom_range=0.1,  # Randomly zoom in on images by up to 10% to simulate different scales
    horizontal_flip=False,  # Do not flip images horizontally as traffic signs should remain in their original orientation
    fill_mode='nearest'  # Fill in any empty space created by transformations using the nearest pixel values
)

# Fit the data augmentation generator on the training data
datagen.fit(X_train_split)  # Adapt the generator to the training data for consistent transformations

# Compute class weights to address class imbalance in the dataset
class_weights = compute_class_weight(
    class_weight='balanced',  # Automatically adjust weights inversely proportional to class frequencies
    classes=np.unique(y_train),  # Unique classes in the training labels
    y=y_train  # Training labels to calculate weights
)
class_weights_dict = dict(enumerate(class_weights))  # Convert the array of class weights to a dictionary format

# Standardize the dataset to have zero mean and unit variance
mean = np.mean(X_train, axis=(0, 1, 2), keepdims=True)  # Calculate the mean pixel value across the training set
std = np.std(X_train, axis=(0, 1, 2), keepdims=True)  # Calculate the standard deviation of pixel values across the training set
X_train = (X_train - mean) / std  # Subtract the mean and divide by the standard deviation for the training set
X_test = (X_test - mean) / std  # Apply the same transformation to the test set for consistency

# Check and print the class distribution in the training data to understand any imbalance
print("Class distribution in training data:")
print(pd.Series(y_train).value_counts())  # Print the count of each class in the training labels

"""- end of preprocessing
- Bigenning of training
"""

#Function for Plotting Performance
# Performance Plot
def plot_performance(history = None, figure_directory = None, ylim_pad = [0, 0]):
    xlabel = 'Epoch'
    legends = ['Training', 'Validation']

#Defining size of graph/plot
    plt.figure(figsize = (20, 5))

#Collecting epoch history data
    y1 = history.history['accuracy']
    y2 = history.history['val_accuracy']

#assigning minimum and maximum y values for the graph
    min_y = min(min(y1), min(y2)) - ylim_pad[0]
    max_y = max(max(y1), max(y2)) + ylim_pad[0]


    plt.subplot(121)

    plt.plot(y1)
    plt.plot(y2)

    plt.title('Model Accuracy\n', fontsize = 17)
    plt.xlabel(xlabel, fontsize = 15)
    plt.ylabel('Accuracy', fontsize = 15)
    plt.ylim(min_y, max_y)
    plt.legend(legends, loc = 'upper left')

    plt.grid()

    y1 = history.history['loss']
    y2 = history.history['val_loss']

    min_y = min(min(y1), min(y2)) - ylim_pad[1]
    max_y = max(max(y1), max(y2)) + ylim_pad[1]


    plt.subplot(122)

    plt.plot(y1)
    plt.plot(y2)

    plt.title('Model Loss\n', fontsize = 17)
    plt.xlabel(xlabel, fontsize = 15)
    plt.ylabel('Loss', fontsize = 15)
    plt.ylim(min_y, max_y)
    plt.legend(legends, loc = 'upper left')
    plt.grid()
    if figure_directory:
        plt.savefig(figure_directory + "/history")

    plt.show()

# Building the convolutional neural network (CNN) model
model = Sequential()  # Initialize a sequential model

# Add a 2D convolutional layer with 32 filters, a kernel size of 5x5, ReLU activation, and input shape matching the training data dimensions
model.add(Conv2D(filters=32, kernel_size=(5, 5), activation='relu', input_shape=X_train_split.shape[1:]))

# Add another 2D convolutional layer with 64 filters, a kernel size of 5x5, and ReLU activation
model.add(Conv2D(filters=64, kernel_size=(5, 5), activation='relu'))

# Add a max pooling layer to reduce spatial dimensions (downsampling) with a pool size of 2x2
model.add(MaxPool2D(pool_size=(2, 2)))

# Add a dropout layer with a dropout rate of 0.15 to prevent overfitting by randomly setting 15% of input units to 0
model.add(Dropout(rate=0.15))

# Add another 2D convolutional layer with 128 filters, a kernel size of 3x3, and ReLU activation
model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))

# Add another 2D convolutional layer with 256 filters, a kernel size of 3x3, and ReLU activation
model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))

# Add a second max pooling layer to further reduce spatial dimensions with a pool size of 2x2
model.add(MaxPool2D(pool_size=(2, 2)))

# Add another dropout layer with a dropout rate of 0.20 to prevent overfitting by randomly setting 20% of input units to 0
model.add(Dropout(rate=0.20))

# Add a flattening layer to convert the 2D matrices into a 1D vector for the dense layers
model.add(Flatten())

# Add a dense (fully connected) layer with 512 units and ReLU activation
model.add(Dense(512, activation='relu'))

# Add a dropout layer with a dropout rate of 0.25 to prevent overfitting by randomly setting 25% of input units to 0
model.add(Dropout(rate=0.25))

# Add another dense layer with 256 units and ReLU activation
model.add(Dense(256, activation='relu'))

# Add another dropout layer with a dropout rate of 0.25 to prevent overfitting by randomly setting 25% of input units to 0
model.add(Dropout(rate=0.25))

# Add the output layer with 43 units (number of classes) and softmax activation for multi-class classification
model.add(Dense(43, activation='softmax'))

# Compilation of the model
model.compile(
    loss='categorical_crossentropy',  # Define the loss function as categorical crossentropy for multi-class classification
    optimizer='adam',  # Use the Adam optimizer for training
    metrics=['accuracy', 'precision', 'f1_score']  # Track accuracy, precision, and F1 score during training and evaluation
)

# Displaying the model's architecture
model.summary()  # Print a summary of the model's architecture, including the layer types, output shapes, and number of parameters

# Define early stopping criterion to prevent overfitting
early_stopping = EarlyStopping(
    monitor='accuracy',  # Monitor the 'accuracy' metric to decide when to stop training
    min_delta=0.01,  # Minimum change in the monitored metric to qualify as an improvement (0.01 means 1% improvement)
    patience=2,  # Number of epochs with no improvement after which training will be stopped (wait for 2 epochs with no improvement)
    restore_best_weights=True,  # Restore the model weights from the epoch with the best accuracy once training stops
)

# Training the model
with tf.device('/GPU:1'):
    epochs = 5
    history1 = model.fit(X_train_split, y_train_split, batch_size = 128, epochs = epochs, validation_data = (X_val_split, y_val_split))

#Visualize Training Performance
plot_performance(history = history1)

# Importing the test dataset

labels = np.argmax(y_test_encoded, axis=1)

# Set the TensorFlow device to GPU for model prediction
with tf.device('/GPU:0'):  # Use the GPU with identifier '/GPU:0' for performing predictions
    pred = np.argmax(model.predict(X_test), axis=-1)  # Predict the class probabilities for the test set and return the class with the highest probability for each sample

# Calculate and display accuracy, precision, and F1 score for the test data
print("Accuracy Score:", accuracy_score(labels, pred))  # Compute and print the accuracy score, which measures the proportion of correctly predicted labels to the total number of labels
print("Precision Score:", precision_score(labels, pred, average='macro'))  # Compute and print the precision score, which measures the ability of the model to correctly identify positive instances (using a macro average to compute the metric independently for each class and then take the average)
print("F1 Score:", f1_score(labels, pred, average='macro'))  # Compute and print the F1 score, which is the harmonic mean of precision and recall (using a macro average to compute the metric independently for each class and then take the average)

# Generate and display the Confusion Matrix for model evaluation
print("\nConfusion Matrix\n")
confMatrix = confusion_matrix(labels, pred)  # Compute the confusion matrix using true labels and predicted labels
print(confMatrix, "\n")  # Print the confusion matrix to the console

# Extract the unique class labels from both true and predicted labels
all_labels = np.unique(np.concatenate([labels, pred]))  # Combine the labels and predictions to find all unique classes

# Create a figure for the confusion matrix heatmap
plt.figure(figsize=(10, 8))  # Set the figure size for better visibility
dis = ConfusionMatrixDisplay(confusion_matrix=confMatrix, display_labels=[str(i) for i in all_labels])  # Create a display object for the confusion matrix with labeled classes
sns.heatmap(confMatrix, annot=True, fmt='d', cmap='Blues', xticklabels=meta_df['ClassId'], yticklabels=meta_df['ClassId'])  # Plot the confusion matrix as a heatmap with annotations and blue color map
dis.plot(cmap=plt.cm.Blues)  # Plot the confusion matrix display object with a blue color map
plt.show()  # Show the confusion matrix plot

# Generate and display a comprehensive classification report
print("Classification Report\n")
print(classification_report(labels, pred))  # Print the classification report to the console, providing detailed metrics

#Testing

class_label ={ 0:'Speed limit (20km/h)',
            1:'Speed limit (30km/h)',
            2:'Speed limit (50km/h)',
            3:'Speed limit (60km/h)',
            4:'Speed limit (70km/h)',
            5:'Speed limit (80km/h)',
            6:'End of speed limit (80km/h)',
            7:'Speed limit (100km/h)',
            8:'Speed limit (120km/h)',
            9:'No passing',
            10:'No passing veh over 3.5 tons',
            11:'Right-of-way at intersection',
            12:'Priority road',
            13:'Yield',
            14:'Stop',
            15:'No vehicles',
            16:'Veh > 3.5 tons prohibited',
            17:'No entry',
            18:'General caution',
            19:'Dangerous curve left',
            20:'Dangerous curve right',
            21:'Double curve',
            22:'Bumpy road',
            23:'Slippery road',
            24:'Road narrows on the right',
            25:'Road work',
            26:'Traffic signals',
            27:'Pedestrians',
            28:'Children crossing',
            29:'Bicycles crossing',
            30:'Beware of ice/snow',
            31:'Wild animals crossing',
            32:'End speed + passing limits',
            33:'Turn right ahead',
            34:'Turn left ahead',
            35:'Ahead only',
            36:'Go straight or right',
            37:'Go straight or left',
            38:'Keep right',
            39:'Keep left',
            40:'Roundabout mandatory',
            41:'End of no passing',
            42:'End no passing veh > 3.5 tons' }

print("Testing On Set Images: \n \n")
# Loop through each image in the folder
for i in range(5):
  #for lb, im in zip(labels, X_test):
      im = X_test[i]
      lb = labels[i]

      # Load and preprocess the image
      test_image = np.expand_dims(im, axis=0)

      # Normalize and make prediction
      result = model.predict(test_image)

      # Get the predicted class index
      prediction_index = np.argmax(result)

      # Display the image
      plt.imshow(im)
      plt.axis('off')
      plt.title(f"Prediction: {prediction_index}")
      plt.show()

      print(f"Prediction for {lb}:\033[1m {class_label[int(prediction_index)]}\033[0m")
      print("===========================================================")

print("Testing On Random Images: \n \n")
for i in range(5):
  random_index = random.randint(1, len(X_test))
  random_image = X_test[random_index]
  random_label = labels[random_index]

  # Load and preprocess the image
  test_image = np.expand_dims(random_image, axis=0)

  # Normalize and make prediction
  result = model.predict(test_image)

  # Get the predicted class index
  prediction_index = np.argmax(result)

  # Display the image
  plt.imshow(random_image)
  plt.axis('off')
  plt.title(f"Prediction: {prediction_index}")
  plt.show()

  print(f"Prediction for {random_label}:\033[1m {class_label[int(prediction_index)]}\033[0m")
  print("===========================================================")